{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "46467e21",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucas6028/aortic_valve_detection/blob/main/train_faster_rcnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de429784",
      "metadata": {
        "id": "de429784"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucas6028/aortic_valve_detection/blob/faster-r-cnn/train_faster_rcnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c44739cc",
      "metadata": {
        "id": "c44739cc"
      },
      "source": [
        "# üî¨ AI CUP 2025 - Aortic Valve Detection with Faster R-CNN\n",
        "\n",
        "## üéØ Faster R-CNN Implementation for Medical Imaging\n",
        "\n",
        "Êú¨ notebook ÂØ¶Áèæ‰∫ÜÂü∫Êñº **Faster R-CNN** ÁöÑ‰∏ªÂãïËÑàÁì£Ê™¢Ê∏¨Ê®°ÂûãÔºåÂèÉËÄÉ RSNA Pneumonia Detection Challenge Á¨¨‰∏ÄÂêçËß£Ê≥ï„ÄÇ\n",
        "\n",
        "### üìö ÂèÉËÄÉ‰æÜÊ∫ê\n",
        "\n",
        "1. **RSNA Pneumonia Detection Challenge** (1st place solution - Ian Pan & Alexandre Cadrin-Ch√™nevert)\n",
        "   - Faster R-CNN with ResNet-50/101 backbone\n",
        "   - Multi-scale training and test-time augmentation\n",
        "   - Hard negative mining for class imbalance\n",
        "   - Ensemble of multiple models\n",
        "\n",
        "2. **Key Advantages of Faster R-CNN for Medical Imaging**:\n",
        "   - Better localization accuracy (important for clinical use)\n",
        "   - Region Proposal Network (RPN) learns optimal anchors\n",
        "   - ROI Pooling preserves spatial information\n",
        "   - Pre-trained ImageNet weights transfer well\n",
        "\n",
        "### ‚ú® ‰∏ªË¶ÅÁâπËâ≤\n",
        "\n",
        "- ‚úÖ **Faster R-CNN with FPN V2**: Enhanced Feature Pyramid Network for multi-scale detection\n",
        "- ‚úÖ **ResNet-50-FPN-V2 Backbone**: Pre-trained on COCO, fine-tuned on medical images\n",
        "- ‚úÖ **Improved Architecture**: 2-3% better mAP than V1, superior localization accuracy\n",
        "- ‚úÖ **Multi-scale Training**: [512, 640, 768, 896] for robust detection\n",
        "- ‚úÖ **Data Augmentation**: Horizontal flip, brightness, contrast adjustments\n",
        "- ‚úÖ **Class Imbalance Handling**: Balanced sampling + focal loss option\n",
        "- ‚úÖ **Test-Time Augmentation**: Multi-scale + flip for inference\n",
        "- ‚úÖ **Model Ensemble**: Average predictions from multiple checkpoints\n",
        "\n",
        "### üìä Expected Performance\n",
        "\n",
        "- **AP@0.5**: Target 0.95-0.97 (improvement over YOLOv8 baseline)\n",
        "- **Recall**: 0.85-0.95 (reduce false negatives)\n",
        "- **Precision**: 0.80-0.90 (control false positives)\n",
        "\n",
        "### üöÄ Quick Start\n",
        "\n",
        "1. Ensure GPU is available (T4 or better recommended)\n",
        "2. Execute all cells in order\n",
        "3. Training checkpoints saved to Google Drive\n",
        "4. Inference with TTA and ensemble\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d9bb3b",
      "metadata": {
        "id": "04d9bb3b"
      },
      "source": [
        "## 1. Áí∞Â¢ÉË®≠ÁΩÆ (Environment Setup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "54936b8b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54936b8b",
        "outputId": "88f300eb-4a80-4f51-f01e-312a73a3a20a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Nov 12 13:41:52 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b54be70f",
      "metadata": {
        "id": "b54be70f"
      },
      "outputs": [],
      "source": [
        "# Fix encoding issues\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale=True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8562bd35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8562bd35",
        "outputId": "8c95e563-9f16-49c1-c52d-b5b908c18b32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.12/dist-packages (2.0.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pycocotools) (2.0.2)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (2.0.8)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations) (6.0.3)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.11.10)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from albumentations) (4.12.0.88)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (4.2.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (6.5.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python-headless) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision\n",
        "!pip install pycocotools\n",
        "!pip install albumentations\n",
        "!pip install opencv-python-headless\n",
        "!pip install pandas matplotlib seaborn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b8e76be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b8e76be",
        "outputId": "27002e83-4b5e-45ac-f336-c10c5283a63a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Checkpoint directory: /content/drive/MyDrive/AI_CUP_2025/faster_rcnn_checkpoints_fold5\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive for checkpoint storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create checkpoint directory\n",
        "import os\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/AI_CUP_2025/faster_rcnn_checkpoints'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5f9658e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f9658e2",
        "outputId": "ba27605b-9318-45c1-f0fe-14da98888919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Found 7 checkpoint(s):\n",
            "  - best_model.pth (329.6 MB)\n",
            "  - checkpoint_epoch_1.pth (329.6 MB)\n",
            "  - checkpoint_epoch_2.pth (329.6 MB)\n",
            "  - checkpoint_epoch_3.pth (329.6 MB)\n",
            "  - checkpoint_epoch_4.pth (329.6 MB)\n",
            "  - checkpoint_epoch_5.pth (329.6 MB)\n",
            "  - checkpoint_epoch_6.pth (329.6 MB)\n"
          ]
        }
      ],
      "source": [
        "# üîç Optional: Check existing checkpoints in Google Drive\n",
        "import os\n",
        "\n",
        "if os.path.exists(CHECKPOINT_DIR):\n",
        "    checkpoint_files = [f for f in os.listdir(CHECKPOINT_DIR) if f.endswith('.pth')]\n",
        "    if checkpoint_files:\n",
        "        print(f\"‚úÖ Found {len(checkpoint_files)} checkpoint(s):\")\n",
        "        for f in sorted(checkpoint_files):\n",
        "            file_path = os.path.join(CHECKPOINT_DIR, f)\n",
        "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
        "            print(f\"  - {f} ({size_mb:.1f} MB)\")\n",
        "    else:\n",
        "        print(\"üìù No checkpoints found yet (fresh training)\")\n",
        "else:\n",
        "    print(f\"üìÅ Checkpoint directory will be created: {CHECKPOINT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee10746f",
      "metadata": {
        "id": "ee10746f"
      },
      "source": [
        "## 2. ‰∏ãËºâË≥áÊñôÈõÜ (Download Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b04bc25a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "b04bc25a",
        "outputId": "bf239931-8632-4fd6-b30f-9fabf6f8b7ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?export=download&id=1vd2Au7S6RSVXz-ZWIza21vHQyd5_KNx1\n",
            "From (redirected): https://drive.google.com/uc?export=download&id=1vd2Au7S6RSVXz-ZWIza21vHQyd5_KNx1&confirm=t&uuid=4c9a4330-8ca9-4230-a697-a44496fd494f\n",
            "To: /content/training_image.zip\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.83G/1.83G [00:24<00:00, 74.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1fsRkC0YAWXdxZhYiXPqPvPJqXhrZCNz3\n",
            "To: /content/training_label.zip\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 659k/659k [00:00<00:00, 7.09MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/training_label.zip'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download dataset\n",
        "import gdown\n",
        "\n",
        "# Download training images\n",
        "gdown.download(\n",
        "    \"https://drive.google.com/uc?export=download&id=1vd2Au7S6RSVXz-ZWIza21vHQyd5_KNx1\",\n",
        "    \"/content/training_image.zip\"\n",
        ")\n",
        "\n",
        "# Download training labels\n",
        "gdown.download(\n",
        "    \"https://drive.google.com/uc?export=download&id=1fsRkC0YAWXdxZhYiXPqPvPJqXhrZCNz3\",\n",
        "    \"/content/training_label.zip\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c81992d7",
      "metadata": {
        "id": "c81992d7"
      },
      "source": [
        "## 3. Ë≥áÊñôÊ∫ñÂÇôËàáÈ†êËôïÁêÜ (Data Preparation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "afb26b91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afb26b91",
        "outputId": "999eb9d4-11e3-4485-8a27-349640507b61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IMG_ROOT = ./training_image/training_image\n",
            "LBL_ROOT = ./training_label/training_label\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "def find_patient_root(root):\n",
        "    \"\"\"Find directory containing patient folders\"\"\"\n",
        "    for dirpath, dirnames, filenames in os.walk(root):\n",
        "        if any(d.startswith(\"patient\") for d in dirnames):\n",
        "            return dirpath\n",
        "    return root\n",
        "\n",
        "# Extract datasets\n",
        "if not os.path.isdir(\"./training_image\") and os.path.exists(\"training_image.zip\"):\n",
        "    os.makedirs(\"./training_image\", exist_ok=True)\n",
        "    !unzip -q training_image.zip -d ./training_image\n",
        "\n",
        "if not os.path.isdir(\"./training_label\") and os.path.exists(\"training_label.zip\"):\n",
        "    os.makedirs(\"./training_label\", exist_ok=True)\n",
        "    !unzip -q training_label.zip -d ./training_label\n",
        "\n",
        "IMG_ROOT = find_patient_root(\"./training_image\")\n",
        "LBL_ROOT = find_patient_root(\"./training_label\")\n",
        "\n",
        "print(\"IMG_ROOT =\", IMG_ROOT)\n",
        "print(\"LBL_ROOT =\", LBL_ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "746e43e9",
      "metadata": {
        "id": "746e43e9"
      },
      "outputs": [],
      "source": [
        "def yolo_to_coco_bbox(yolo_box, img_width, img_height):\n",
        "    \"\"\"\n",
        "    Convert YOLO format (xc, yc, w, h) normalized to COCO format (x_min, y_min, w, h) in pixels\n",
        "    \"\"\"\n",
        "    xc, yc, w, h = yolo_box\n",
        "\n",
        "    # Convert to pixel coordinates\n",
        "    xc_px = xc * img_width\n",
        "    yc_px = yc * img_height\n",
        "    w_px = w * img_width\n",
        "    h_px = h * img_height\n",
        "\n",
        "    # Convert center to top-left\n",
        "    x_min = xc_px - w_px / 2\n",
        "    y_min = yc_px - h_px / 2\n",
        "\n",
        "    return [x_min, y_min, w_px, h_px]\n",
        "\n",
        "def prepare_coco_dataset(img_root, lbl_root, output_dir, split_name, sample_list):\n",
        "    \"\"\"\n",
        "    Prepare dataset in COCO format for Faster R-CNN\n",
        "\n",
        "    Args:\n",
        "        sample_list: list of tuples (patient_dir, img_file, label_path or None)\n",
        "    \"\"\"\n",
        "    images_dir = os.path.join(output_dir, split_name, 'images')\n",
        "    os.makedirs(images_dir, exist_ok=True)\n",
        "\n",
        "    # Complete COCO format with all required fields\n",
        "    coco_dict = {\n",
        "        'info': {\n",
        "            'description': 'AI CUP 2025 Aortic Valve Detection Dataset',\n",
        "            'version': '1.0',\n",
        "            'year': 2025,\n",
        "            'contributor': 'AI CUP 2025',\n",
        "            'date_created': '2025-10-23'\n",
        "        },\n",
        "        'licenses': [{\n",
        "            'id': 1,\n",
        "            'name': 'Unknown',\n",
        "            'url': ''\n",
        "        }],\n",
        "        'images': [],\n",
        "        'annotations': [],\n",
        "        'categories': [{'id': 1, 'name': 'aortic_valve', 'supercategory': 'medical'}]\n",
        "    }\n",
        "\n",
        "    image_id = 1\n",
        "    annotation_id = 1\n",
        "\n",
        "    for patient_dir, img_file, label_path in sample_list:\n",
        "        # Copy image\n",
        "        src_img = os.path.join(img_root, patient_dir, img_file)\n",
        "        if not os.path.exists(src_img):\n",
        "            continue\n",
        "\n",
        "        # Read image to get dimensions\n",
        "        img = cv2.imread(src_img)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        height, width = img.shape[:2]\n",
        "\n",
        "        # Create unique filename\n",
        "        new_filename = f\"{patient_dir}_{img_file}\"\n",
        "        dst_img = os.path.join(images_dir, new_filename)\n",
        "        shutil.copy2(src_img, dst_img)\n",
        "\n",
        "        # Add image info (with all COCO required fields)\n",
        "        coco_dict['images'].append({\n",
        "            'id': image_id,\n",
        "            'file_name': new_filename,\n",
        "            'width': width,\n",
        "            'height': height,\n",
        "            'license': 1,\n",
        "            'flickr_url': '',\n",
        "            'coco_url': '',\n",
        "            'date_captured': ''\n",
        "        })\n",
        "\n",
        "        # Add annotations if label exists\n",
        "        if label_path and os.path.exists(label_path):\n",
        "            try:\n",
        "                with open(label_path, 'r') as f:\n",
        "                    for line in f:\n",
        "                        parts = line.strip().split()\n",
        "                        if len(parts) == 5:\n",
        "                            class_id, xc, yc, w, h = map(float, parts)\n",
        "\n",
        "                            # Convert to COCO format\n",
        "                            bbox = yolo_to_coco_bbox([xc, yc, w, h], width, height)\n",
        "                            area = bbox[2] * bbox[3]\n",
        "\n",
        "                            coco_dict['annotations'].append({\n",
        "                                'id': annotation_id,\n",
        "                                'image_id': image_id,\n",
        "                                'category_id': 1,\n",
        "                                'bbox': bbox,\n",
        "                                'area': area,\n",
        "                                'iscrowd': 0\n",
        "                            })\n",
        "                            annotation_id += 1\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        image_id += 1\n",
        "\n",
        "    # Save COCO annotation file\n",
        "    anno_file = os.path.join(output_dir, split_name, 'annotations.json')\n",
        "    with open(anno_file, 'w') as f:\n",
        "        json.dump(coco_dict, f)\n",
        "\n",
        "    print(f\"{split_name}: {len(coco_dict['images'])} images, {len(coco_dict['annotations'])} annotations\")\n",
        "    return anno_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "15457713",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15457713",
        "outputId": "40bcf42d-0350-4e6c-e0fc-48f94a657886"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Analyzing dataset distribution...\n",
            "Total samples: 16863\n",
            "Positive samples (with aortic valve): 2787 (16.5%)\n",
            "Negative samples (background): 14076 (83.5%)\n"
          ]
        }
      ],
      "source": [
        "# üìä Analyze dataset distribution\n",
        "print(\"\\nüìä Analyzing dataset distribution...\")\n",
        "positive_samples = []  # (patient, image_name, label_path)\n",
        "negative_samples = []  # (patient, image_name, None)\n",
        "\n",
        "for patient_dir in sorted(os.listdir(IMG_ROOT)):\n",
        "    if not patient_dir.startswith(\"patient\"):\n",
        "        continue\n",
        "\n",
        "    img_dir = os.path.join(IMG_ROOT, patient_dir)\n",
        "    lbl_dir = os.path.join(LBL_ROOT, patient_dir)\n",
        "\n",
        "    if not os.path.isdir(img_dir):\n",
        "        continue\n",
        "\n",
        "    for img_file in os.listdir(img_dir):\n",
        "        if not img_file.lower().endswith('.png'):\n",
        "            continue\n",
        "\n",
        "        base_name = os.path.splitext(img_file)[0]\n",
        "        label_path = os.path.join(lbl_dir, base_name + '.txt')\n",
        "\n",
        "        # Check if positive (has label file with content)\n",
        "        is_positive = False\n",
        "        if os.path.exists(label_path):\n",
        "            try:\n",
        "                with open(label_path, 'r') as f:\n",
        "                    content = f.read().strip()\n",
        "                    if content:\n",
        "                        is_positive = True\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        if is_positive:\n",
        "            positive_samples.append((patient_dir, img_file, label_path))\n",
        "        else:\n",
        "            negative_samples.append((patient_dir, img_file, None))\n",
        "\n",
        "total = len(positive_samples) + len(negative_samples)\n",
        "pos_ratio = len(positive_samples) / total * 100 if total > 0 else 0\n",
        "\n",
        "print(f\"Total samples: {total}\")\n",
        "print(f\"Positive samples (with aortic valve): {len(positive_samples)} ({pos_ratio:.1f}%)\")\n",
        "print(f\"Negative samples (background): {len(negative_samples)} ({100-pos_ratio:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4d02ca1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4d02ca1",
        "outputId": "9877166c-7cde-425b-c7e4-a9d17cd00253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîÄ Creating train/val split (same as K-Fold Fold 1)...\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Split matches K-Fold Fold 1:\n",
            "  Train: 2230 pos + 11261 neg = 13491 (16.5% positive)\n",
            "  Val:   557 pos + 2815 neg = 3372 (16.5% positive)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# üì¶ Stratified split (80% train, 20% val)\n",
        "random.seed(42)\n",
        "random.shuffle(positive_samples)\n",
        "random.shuffle(negative_samples)\n",
        "\n",
        "# Calculate split sizes\n",
        "n_pos = len(positive_samples)\n",
        "n_neg = len(negative_samples)\n",
        "\n",
        "train_pos = int(n_pos * 0.80)\n",
        "train_neg = int(n_neg * 0.80)\n",
        "\n",
        "# Split samples\n",
        "train_samples = positive_samples[:train_pos] + negative_samples[:train_neg]\n",
        "val_samples = positive_samples[train_pos:] + negative_samples[train_neg:]\n",
        "\n",
        "random.shuffle(train_samples)\n",
        "random.shuffle(val_samples)\n",
        "\n",
        "print(f\"\\nTrain: {len(train_samples)} samples\")\n",
        "print(f\"Val: {len(val_samples)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ecba766",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ecba766",
        "outputId": "05d157ec-39bf-421c-8616-4eb9f1648089"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: 13491 images, 2230 annotations\n",
            "val: 3372 images, 557 annotations\n",
            "\n",
            "‚úÖ COCO dataset prepared in ./datasets_coco\n",
            "   - Train set matches K-Fold Fold 1 training set (80% of data)\n",
            "   - Val set matches K-Fold Fold 1 validation set (20% of data)\n"
          ]
        }
      ],
      "source": [
        "# üîÑ Prepare COCO format datasets\n",
        "OUTPUT_DIR = './datasets_coco'\n",
        "\n",
        "train_anno = prepare_coco_dataset(IMG_ROOT, LBL_ROOT, OUTPUT_DIR, 'train', train_samples)\n",
        "val_anno = prepare_coco_dataset(IMG_ROOT, LBL_ROOT, OUTPUT_DIR, 'val', val_samples)\n",
        "\n",
        "print(f\"\\n‚úÖ COCO dataset prepared in {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0bd8f10",
      "metadata": {
        "id": "e0bd8f10"
      },
      "source": [
        "## 4. ÂÆöÁæ© Faster R-CNN Ê®°Âûã (Define Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5dfc83de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dfc83de",
        "outputId": "eb4ef0eb-933b-4a46-ab96-06fb797d4359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167M/167M [00:00<00:00, 194MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Faster R-CNN V2 model created successfully\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "def get_model(num_classes=2, pretrained=True):\n",
        "    \"\"\"\n",
        "    Create Faster R-CNN model with ResNet-50-FPN-V2 backbone\n",
        "\n",
        "    V2 improvements over V1:\n",
        "    - Better Feature Pyramid Network with improved multi-scale fusion\n",
        "    - 2-3% higher mAP on COCO dataset\n",
        "    - Better localization accuracy (important for medical imaging)\n",
        "    - More robust to class imbalance\n",
        "\n",
        "    Args:\n",
        "        num_classes: Number of classes (background + aortic_valve = 2)\n",
        "        pretrained: Use COCO pre-trained weights\n",
        "    \"\"\"\n",
        "    # Load pre-trained Faster R-CNN V2 (using modern weights API)\n",
        "    if pretrained:\n",
        "        model = fasterrcnn_resnet50_fpn_v2(weights='DEFAULT')\n",
        "    else:\n",
        "        model = fasterrcnn_resnet50_fpn_v2(weights=None)\n",
        "\n",
        "    # Get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    # Replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Test model creation\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = get_model(num_classes=2)\n",
        "model.to(device)\n",
        "print(\"‚úÖ Faster R-CNN V2 model created successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afe56173",
      "metadata": {
        "id": "afe56173"
      },
      "source": [
        "### üÜï Why FPN V2 for Medical Imaging?\n",
        "\n",
        "**Key improvements in V2 over V1:**\n",
        "\n",
        "1. **Better Feature Pyramid Network**\n",
        "   - Enhanced multi-scale feature fusion\n",
        "   - More effective at detecting objects of varying sizes\n",
        "   - Critical for aortic valves that appear at different scales in CT scans\n",
        "\n",
        "2. **Superior Performance**\n",
        "   - **+2-3% higher mAP** on COCO benchmark\n",
        "   - Better AP@0.75 (stricter IoU threshold)\n",
        "   - Improved localization accuracy\n",
        "\n",
        "3. **Medical Imaging Benefits**\n",
        "   - Better handling of 512√ó512 grayscale images\n",
        "   - More robust to class imbalance (~20% positive samples)\n",
        "   - Improved small object detection precision\n",
        "\n",
        "4. **Modern Architecture**\n",
        "   - Updated to latest PyTorch best practices\n",
        "   - Better gradient flow during training\n",
        "   - Uses `weights='DEFAULT'` API (replaces deprecated `pretrained=True`)\n",
        "\n",
        "**Expected improvements for this project:**\n",
        "- Higher AP@0.5 (competition metric)\n",
        "- Better precision on challenging valve detections\n",
        "- More stable training convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02db0b7c",
      "metadata": {
        "id": "02db0b7c"
      },
      "source": [
        "### üìù Note on COCO Format\n",
        "\n",
        "The dataset preparation creates **complete COCO format** annotations with all required fields:\n",
        "- `info`: Dataset metadata (description, version, year)\n",
        "- `licenses`: License information\n",
        "- `images`: Image metadata (id, filename, width, height, license)\n",
        "- `annotations`: Bounding box annotations (bbox, category_id, area)\n",
        "- `categories`: Class definitions (id, name, supercategory)\n",
        "\n",
        "This ensures compatibility with COCO evaluation API and prevents `KeyError` issues during validation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b776c73",
      "metadata": {
        "id": "1b776c73"
      },
      "source": [
        "## 5. Ë≥áÊñôËºâÂÖ•Âô®ËàáÂ¢ûÂº∑ (Data Loader & Augmentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "166f8772",
      "metadata": {
        "id": "166f8772"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pycocotools.coco import COCO\n",
        "import PIL.Image as Image\n",
        "\n",
        "class AorticValveDataset(Dataset):\n",
        "    def __init__(self, root, annotation_file, transforms=None):\n",
        "        self.root = root\n",
        "        self.coco = COCO(annotation_file)\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        coco = self.coco\n",
        "        img_id = self.ids[index]\n",
        "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
        "        anns = coco.loadAnns(ann_ids)\n",
        "\n",
        "        # Load image\n",
        "        img_info = coco.loadImgs(img_id)[0]\n",
        "        path = os.path.join(self.root, 'images', img_info['file_name'])\n",
        "        img = cv2.imread(path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Get bboxes and labels\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        for ann in anns:\n",
        "            xmin, ymin, width, height = ann['bbox']\n",
        "            boxes.append([xmin, ymin, xmin + width, ymin + height])\n",
        "            labels.append(ann['category_id'])\n",
        "\n",
        "        # If no annotations, create empty tensors\n",
        "        if len(boxes) == 0:\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros((0,), dtype=torch.int64)\n",
        "        else:\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        image_id = torch.tensor([img_id])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if len(boxes) > 0 else torch.zeros((0,), dtype=torch.float32)\n",
        "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'image_id': image_id,\n",
        "            'area': area,\n",
        "            'iscrowd': iscrowd\n",
        "        }\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transforms:\n",
        "            transformed = self.transforms(\n",
        "                image=img,\n",
        "                bboxes=boxes.numpy() if len(boxes) > 0 else [],\n",
        "                labels=labels.numpy() if len(labels) > 0 else []\n",
        "            )\n",
        "            img = transformed['image']\n",
        "\n",
        "            if len(transformed['bboxes']) > 0:\n",
        "                target['boxes'] = torch.as_tensor(transformed['bboxes'], dtype=torch.float32)\n",
        "                target['labels'] = torch.as_tensor(transformed['labels'], dtype=torch.int64)\n",
        "                target['area'] = (target['boxes'][:, 3] - target['boxes'][:, 1]) * (target['boxes'][:, 2] - target['boxes'][:, 0])\n",
        "\n",
        "        # Convert image to tensor and normalize\n",
        "        img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "47256ccb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47256ccb",
        "outputId": "29cf66fe-6a42-4998-b92e-b427de4edf0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.04s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Train dataset: 13491 samples\n",
            "Val dataset: 3372 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3960439324.py:6: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
            "/usr/local/lib/python3.12/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/albumentations/core/composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
            "  self._set_keys()\n"
          ]
        }
      ],
      "source": [
        "# Define augmentation transforms\n",
        "def get_train_transform():\n",
        "    return A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
        "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n",
        "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "\n",
        "def get_val_transform():\n",
        "    return A.Compose([\n",
        "        # No augmentation for validation\n",
        "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = AorticValveDataset(\n",
        "    root=os.path.join(OUTPUT_DIR, 'train'),\n",
        "    annotation_file=train_anno,\n",
        "    transforms=get_train_transform()\n",
        ")\n",
        "\n",
        "val_dataset = AorticValveDataset(\n",
        "    root=os.path.join(OUTPUT_DIR, 'val'),\n",
        "    annotation_file=val_anno,\n",
        "    transforms=get_val_transform()\n",
        ")\n",
        "\n",
        "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
        "print(f\"Val dataset: {len(val_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2a51883f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a51883f",
        "outputId": "2dc7ee83-e30a-4ac8-9e2d-60707832cd97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train batches: 1687\n",
            "Val batches: 422\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "# Create data loaders\n",
        "BATCH_SIZE = 8  # Adjust based on GPU memory\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8a4276b",
      "metadata": {
        "id": "b8a4276b"
      },
      "source": [
        "## 6. Ë®ìÁ∑¥ÂáΩÊï∏ (Training Functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "03924ac5",
      "metadata": {
        "id": "03924ac5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    loss_dict_cumulative = {}\n",
        "\n",
        "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch}\")\n",
        "\n",
        "    for images, targets in progress_bar:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate losses\n",
        "        epoch_loss += losses.item()\n",
        "        for k, v in loss_dict.items():\n",
        "            if k not in loss_dict_cumulative:\n",
        "                loss_dict_cumulative[k] = 0\n",
        "            loss_dict_cumulative[k] += v.item()\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({'loss': losses.item()})\n",
        "\n",
        "    # Average losses\n",
        "    num_batches = len(data_loader)\n",
        "    avg_loss = epoch_loss / num_batches\n",
        "    avg_loss_dict = {k: v / num_batches for k, v in loss_dict_cumulative.items()}\n",
        "\n",
        "    return avg_loss, avg_loss_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6ce653b0",
      "metadata": {
        "id": "6ce653b0"
      },
      "outputs": [],
      "source": [
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools.coco import COCO\n",
        "import json\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader, device, coco_gt):\n",
        "    model.eval()\n",
        "\n",
        "    coco_results = []\n",
        "\n",
        "    for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "        images = list(image.to(device) for image in images)\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        for target, output in zip(targets, outputs):\n",
        "            image_id = target['image_id'].item()\n",
        "\n",
        "            boxes = output['boxes'].cpu().numpy()\n",
        "            scores = output['scores'].cpu().numpy()\n",
        "            labels = output['labels'].cpu().numpy()\n",
        "\n",
        "            for box, score, label in zip(boxes, scores, labels):\n",
        "                # Convert to COCO format [x, y, width, height]\n",
        "                x1, y1, x2, y2 = box\n",
        "                width = x2 - x1\n",
        "                height = y2 - y1\n",
        "\n",
        "                coco_results.append({\n",
        "                    'image_id': image_id,\n",
        "                    'category_id': int(label),\n",
        "                    'bbox': [float(x1), float(y1), float(width), float(height)],\n",
        "                    'score': float(score)\n",
        "                })\n",
        "\n",
        "    if len(coco_results) == 0:\n",
        "        print(\"No detections found!\")\n",
        "        return {'AP@0.5': 0.0}\n",
        "\n",
        "    # Evaluate using COCO API\n",
        "    coco_dt = coco_gt.loadRes(coco_results)\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "\n",
        "    # Extract AP@0.5\n",
        "    ap_50 = coco_eval.stats[1]  # AP at IoU=0.50\n",
        "\n",
        "    return {\n",
        "        'AP@0.5': ap_50,\n",
        "        'mAP': coco_eval.stats[0],\n",
        "        'AP@0.75': coco_eval.stats[2]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "517cdda6",
      "metadata": {
        "id": "517cdda6"
      },
      "source": [
        "## 7. Ë®ìÁ∑¥Ê®°Âûã (Train Model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb7b972b",
      "metadata": {
        "id": "fb7b972b"
      },
      "source": [
        "### üìù Resume Training Instructions\n",
        "\n",
        "The training script supports **automatic checkpoint resuming**:\n",
        "\n",
        "- **Set `RESUME_TRAINING = True`** to automatically resume from the latest checkpoint\n",
        "- The script will search for:\n",
        "  1. `best_model.pth` (best model so far)\n",
        "  2. `checkpoint_epoch_{N}.pth` (latest periodic checkpoint)\n",
        "- When resuming, it restores:\n",
        "  - Model weights\n",
        "  - Optimizer state (momentum, learning rate)\n",
        "  - Learning rate scheduler state\n",
        "  - Training history\n",
        "  - Best AP@0.5 score\n",
        "  - Starting epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ee1f0eef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee1f0eef",
        "outputId": "7b000063-7ecc-41c6-a2c1-a6e503f2ef83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Found checkpoint from epoch 6\n",
            "üîÑ Resuming training from: /content/drive/MyDrive/AI_CUP_2025/faster_rcnn_checkpoints_fold5/checkpoint_epoch_6.pth\n",
            "‚úÖ Model weights loaded\n",
            "‚úÖ Optimizer state loaded\n",
            "‚úÖ Best AP@0.5 restored: 0.9640\n",
            "‚úÖ Training history loaded: 1 records\n",
            "‚úÖ Learning rate scheduler adjusted to epoch 6\n",
            "\n",
            "üéØ Resuming from epoch 7/50\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "\n",
            "üöÄ Training Configuration:\n",
            "Device: cuda\n",
            "Total Epochs: 50\n",
            "Starting Epoch: 7\n",
            "Batch size: 8\n",
            "Learning rate: 0.005\n",
            "Best AP so far: 0.9640\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Training configuration\n",
        "NUM_EPOCHS = 50\n",
        "LEARNING_RATE = 0.005\n",
        "WEIGHT_DECAY = 0.0005\n",
        "LR_STEP_SIZE = 10\n",
        "LR_GAMMA = 0.1\n",
        "RESUME_TRAINING = True  # Set to True to resume from checkpoint\n",
        "\n",
        "# Create model\n",
        "model = get_model(num_classes=2)\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(\n",
        "    params,\n",
        "    lr=LEARNING_RATE,\n",
        "    momentum=0.9,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=LR_STEP_SIZE,\n",
        "    gamma=LR_GAMMA\n",
        ")\n",
        "\n",
        "# üîÑ Resume training from checkpoint if available\n",
        "start_epoch = 1\n",
        "best_ap = 0.0\n",
        "training_history = []\n",
        "\n",
        "if RESUME_TRAINING:\n",
        "    # Check for existing checkpoints\n",
        "    checkpoint_files = []\n",
        "    if os.path.exists(CHECKPOINT_DIR):\n",
        "        checkpoint_files = [f for f in os.listdir(CHECKPOINT_DIR) if f.endswith('.pth')]\n",
        "\n",
        "    if checkpoint_files:\n",
        "        # Try to look for latest checkpoint first, then load the best model\n",
        "        resume_path = None\n",
        "\n",
        "        # Find latest epoch checkpoint\n",
        "        epoch_checkpoints = [f for f in checkpoint_files if f.startswith('checkpoint_epoch_')]\n",
        "        if epoch_checkpoints:\n",
        "            # Extract epoch numbers and sort\n",
        "            epoch_nums = [int(f.split('_')[-1].split('.')[0]) for f in epoch_checkpoints]\n",
        "            latest_epoch = max(epoch_nums)\n",
        "            resume_path = os.path.join(CHECKPOINT_DIR, f'checkpoint_epoch_{latest_epoch}.pth')\n",
        "            print(f\"üìÇ Found checkpoint from epoch {latest_epoch}\")\n",
        "        elif 'best_model.pth' in checkpoint_files:\n",
        "            resume_path = os.path.join(CHECKPOINT_DIR, 'best_model.pth')\n",
        "            print(\"üìÇ Found best_model.pth\")\n",
        "\n",
        "        if resume_path:\n",
        "            print(f\"üîÑ Resuming training from: {resume_path}\")\n",
        "            checkpoint = torch.load(resume_path, weights_only=False)\n",
        "\n",
        "            # Load model state\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            print(\"‚úÖ Model weights loaded\")\n",
        "\n",
        "            # Load optimizer state\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            print(\"‚úÖ Optimizer state loaded\")\n",
        "\n",
        "            # Restore training state\n",
        "            start_epoch = checkpoint['epoch'] + 1\n",
        "            if 'best_ap' in checkpoint:\n",
        "                best_ap = checkpoint['best_ap']\n",
        "                print(f\"‚úÖ Best AP@0.5 restored: {best_ap:.4f}\")\n",
        "\n",
        "            # Try to load training history if available\n",
        "            history_path = os.path.join(CHECKPOINT_DIR, 'training_history.csv')\n",
        "            if os.path.exists(history_path):\n",
        "                history_df = pd.read_csv(history_path)\n",
        "                training_history = history_df.to_dict('records')\n",
        "                print(f\"‚úÖ Training history loaded: {len(training_history)} records\")\n",
        "\n",
        "            # Adjust learning rate scheduler to current epoch\n",
        "            for _ in range(checkpoint['epoch']):\n",
        "                lr_scheduler.step()\n",
        "            print(f\"‚úÖ Learning rate scheduler adjusted to epoch {checkpoint['epoch']}\")\n",
        "\n",
        "            print(f\"\\nüéØ Resuming from epoch {start_epoch}/{NUM_EPOCHS}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No valid checkpoint found, starting fresh training\")\n",
        "    else:\n",
        "        print(\"üìù No checkpoints found, starting fresh training\")\n",
        "else:\n",
        "    print(\"üìù Starting fresh training (RESUME_TRAINING=False)\")\n",
        "\n",
        "# Load COCO ground truth for evaluation\n",
        "coco_gt = COCO(val_anno)\n",
        "\n",
        "print(f\"\\nüöÄ Training Configuration:\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Total Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Starting Epoch: {start_epoch}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Best AP so far: {best_ap:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be606ed0",
      "metadata": {
        "id": "be606ed0"
      },
      "source": [
        "### üõ†Ô∏è Optional: Manual Checkpoint Management\n",
        "\n",
        "If you need more control over checkpoint loading, you can use this cell instead of automatic resume:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ac9ea7a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac9ea7a4",
        "outputId": "ee701c7e-89e0-4c63-8bc5-5df0dc84f93a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üí° Tip: Uncomment the code above to manually load a specific checkpoint\n"
          ]
        }
      ],
      "source": [
        "# ‚ö†Ô∏è OPTIONAL: Manual checkpoint loading (skip if using automatic resume above)\n",
        "# Uncomment and modify the checkpoint path if you want to load a specific checkpoint\n",
        "\n",
        "\"\"\"\n",
        "MANUAL_CHECKPOINT_PATH = '/content/drive/MyDrive/AI_CUP_2025/faster_rcnn_checkpoints/checkpoint_epoch_30.pth'\n",
        "\n",
        "if os.path.exists(MANUAL_CHECKPOINT_PATH):\n",
        "    print(f\"Loading checkpoint from: {MANUAL_CHECKPOINT_PATH}\")\n",
        "    checkpoint = torch.load(MANUAL_CHECKPOINT_PATH)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    best_ap = checkpoint.get('best_ap', 0.0)\n",
        "\n",
        "    # Adjust LR scheduler\n",
        "    if 'lr_scheduler_state_dict' in checkpoint:\n",
        "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
        "    else:\n",
        "        for _ in range(checkpoint['epoch']):\n",
        "            lr_scheduler.step()\n",
        "\n",
        "    print(f\"‚úÖ Manually loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
        "    print(f\"Resuming from epoch {start_epoch}, Best AP: {best_ap:.4f}\")\n",
        "else:\n",
        "    print(f\"‚ùå Checkpoint not found at {MANUAL_CHECKPOINT_PATH}\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"üí° Tip: Uncomment the code above to manually load a specific checkpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "012b3ab1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "012b3ab1",
        "outputId": "4c15a925-4aab-47c6-9ca6-30efd5858c64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Epoch 7/50\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7:   0%|          | 0/1687 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1687/1687 [1:03:42<00:00,  2.27s/it, loss=0.000142]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training - Loss: 0.0145 (Time: 3822.8s)\n",
            "Loss components: {'loss_classifier': '0.0047', 'loss_box_reg': '0.0089', 'loss_objectness': '0.0004', 'loss_rpn_box_reg': '0.0004'}\n",
            "üíæ Checkpoint saved at epoch 7\n",
            "\n",
            "============================================================\n",
            "Epoch 8/50\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1687/1687 [1:03:48<00:00,  2.27s/it, loss=0.0113]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training - Loss: 0.0139 (Time: 3828.9s)\n",
            "Loss components: {'loss_classifier': '0.0045', 'loss_box_reg': '0.0086', 'loss_objectness': '0.0004', 'loss_rpn_box_reg': '0.0004'}\n",
            "üíæ Checkpoint saved at epoch 8\n",
            "\n",
            "============================================================\n",
            "Epoch 9/50\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9:   1%|‚ñè         | 24/1687 [00:55<1:03:11,  2.28s/it, loss=0.000798]"
          ]
        }
      ],
      "source": [
        "# Import pandas for saving training history\n",
        "import pandas as pd\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Train\n",
        "    start_time = time.time()\n",
        "    train_loss, train_loss_dict = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\nTraining - Loss: {train_loss:.4f} (Time: {train_time:.1f}s)\")\n",
        "    print(\"Loss components:\", {k: f\"{v:.4f}\" for k, v in train_loss_dict.items()})\n",
        "\n",
        "    should_validate = False\n",
        "    if epoch % 5 == 0:\n",
        "        should_validate = True\n",
        "    elif epoch == NUM_EPOCHS:\n",
        "        should_validate = True\n",
        "\n",
        "    if should_validate:\n",
        "        start_time = time.time()\n",
        "        metrics = evaluate(model, val_loader, device, coco_gt)\n",
        "        val_time = time.time() - start_time\n",
        "\n",
        "        print(f\"\\nValidation metrics (Time: {val_time:.1f}s):\")\n",
        "        for k, v in metrics.items():\n",
        "            print(f\"  {k}: {v:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if metrics['AP@0.5'] > best_ap:\n",
        "            best_ap = metrics['AP@0.5']\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_ap': best_ap,\n",
        "                'metrics': metrics\n",
        "            }, os.path.join(CHECKPOINT_DIR, 'best_model.pth'))\n",
        "            print(f\"‚úÖ Best model saved! AP@0.5: {best_ap:.4f}\")\n",
        "\n",
        "        training_history.append({\n",
        "            'epoch': epoch,\n",
        "            'train_loss': train_loss,\n",
        "            **metrics\n",
        "        })\n",
        "\n",
        "    # Update learning rate\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    save_checkpoint = True\n",
        "\n",
        "    if save_checkpoint:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'best_ap': best_ap,\n",
        "            'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
        "        }, os.path.join(CHECKPOINT_DIR, f'checkpoint_epoch_{epoch}.pth'))\n",
        "        print(f\"üíæ Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "    # Save training history periodically\n",
        "    if len(training_history) > 0:\n",
        "        pd.DataFrame(training_history).to_csv(\n",
        "            os.path.join(CHECKPOINT_DIR, 'training_history.csv'),\n",
        "            index=False\n",
        "        )\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ Training completed!\")\n",
        "print(f\"Best AP@0.5: {best_ap:.4f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a510d19",
      "metadata": {
        "id": "7a510d19"
      },
      "source": [
        "## 8. Ë¶ñË¶∫ÂåñË®ìÁ∑¥ÁµêÊûú (Visualize Training Results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2916371",
      "metadata": {
        "id": "e2916371"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Convert training history to DataFrame\n",
        "history_df = pd.DataFrame(training_history)\n",
        "\n",
        "# Plot training curves\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Loss\n",
        "axes[0, 0].plot(history_df['epoch'], history_df['train_loss'], marker='o')\n",
        "axes[0, 0].set_title('Training Loss')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].grid(True)\n",
        "\n",
        "# AP@0.5\n",
        "axes[0, 1].plot(history_df['epoch'], history_df['AP@0.5'], marker='o', color='green')\n",
        "axes[0, 1].set_title('AP@0.5 (Target Metric)')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('AP@0.5')\n",
        "axes[0, 1].grid(True)\n",
        "\n",
        "# mAP\n",
        "axes[1, 0].plot(history_df['epoch'], history_df['mAP'], marker='o', color='orange')\n",
        "axes[1, 0].set_title('mAP (0.5:0.95)')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('mAP')\n",
        "axes[1, 0].grid(True)\n",
        "\n",
        "# AP@0.75\n",
        "axes[1, 1].plot(history_df['epoch'], history_df['AP@0.75'], marker='o', color='red')\n",
        "axes[1, 1].set_title('AP@0.75')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('AP@0.75')\n",
        "axes[1, 1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(CHECKPOINT_DIR, 'training_curves.png'), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Training curves saved to:\", os.path.join(CHECKPOINT_DIR, 'training_curves.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "362828a4",
      "metadata": {
        "id": "362828a4"
      },
      "source": [
        "## 9. Ê∏¨Ë©¶ÈõÜÊé®Ë´ñ (Test Set Inference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be4c3616",
      "metadata": {
        "id": "be4c3616"
      },
      "outputs": [],
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load(os.path.join(CHECKPOINT_DIR, 'best_model.pth'), weights_only=False)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"‚úÖ Loaded best model from epoch {checkpoint['epoch']}\")\n",
        "print(f\"Best AP@0.5: {checkpoint['best_ap']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e6f479a",
      "metadata": {
        "id": "8e6f479a"
      },
      "outputs": [],
      "source": [
        "# Create test dataset and loader\n",
        "test_dataset = AorticValveDataset(\n",
        "    root=os.path.join(OUTPUT_DIR, 'test'),\n",
        "    annotation_file=test_anno,\n",
        "    transforms=get_val_transform()\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "coco_test = COCO(test_anno)\n",
        "test_metrics = evaluate(model, test_loader, device, coco_test)\n",
        "\n",
        "print(\"\\nüìä Test Set Results:\")\n",
        "print(\"=\"*40)\n",
        "for k, v in test_metrics.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ea00515",
      "metadata": {
        "id": "4ea00515"
      },
      "source": [
        "## 10. Ë¶ñË¶∫ÂåñÈ†êÊ∏¨ÁµêÊûú (Visualize Predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08827a3b",
      "metadata": {
        "id": "08827a3b"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def visualize_predictions(model, dataset, num_samples=5, conf_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Visualize model predictions on random samples\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, ax in zip(indices, axes):\n",
        "        img, target = dataset[idx]\n",
        "\n",
        "        # Get prediction\n",
        "        with torch.no_grad():\n",
        "            prediction = model([img.to(device)])[0]\n",
        "\n",
        "        # Convert image to numpy\n",
        "        img_np = img.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "        # Plot image\n",
        "        ax.imshow(img_np)\n",
        "\n",
        "        # Plot ground truth boxes (green)\n",
        "        if len(target['boxes']) > 0:\n",
        "            for box in target['boxes']:\n",
        "                x1, y1, x2, y2 = box\n",
        "                rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
        "                                   fill=False, edgecolor='green', linewidth=2)\n",
        "                ax.add_patch(rect)\n",
        "                ax.text(x1, y1-5, 'GT', color='green', fontsize=10, weight='bold')\n",
        "\n",
        "        # Plot predicted boxes (red)\n",
        "        boxes = prediction['boxes'].cpu()\n",
        "        scores = prediction['scores'].cpu()\n",
        "\n",
        "        for box, score in zip(boxes, scores):\n",
        "            if score >= conf_threshold:\n",
        "                x1, y1, x2, y2 = box\n",
        "                rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
        "                                   fill=False, edgecolor='red', linewidth=2)\n",
        "                ax.add_patch(rect)\n",
        "                ax.text(x1, y2+15, f'{score:.2f}', color='red', fontsize=10, weight='bold')\n",
        "\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f'Sample {idx}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(CHECKPOINT_DIR, 'predictions_visualization.png'), dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "# Visualize predictions on test set\n",
        "visualize_predictions(model, test_dataset, num_samples=6, conf_threshold=0.5)\n",
        "print(\"‚úÖ Predictions visualization saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0b90576",
      "metadata": {
        "id": "d0b90576"
      },
      "source": [
        "## 11. ÂÑ≤Â≠òÊúÄÁµÇÁµêÊûú (Save Final Results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ab04bdf",
      "metadata": {
        "id": "6ab04bdf"
      },
      "outputs": [],
      "source": [
        "# Save training history\n",
        "history_df.to_csv(os.path.join(CHECKPOINT_DIR, 'training_history.csv'), index=False)\n",
        "\n",
        "# Save final metrics\n",
        "final_results = {\n",
        "    'model': 'Faster R-CNN ResNet-50-FPN-V2',\n",
        "    'best_epoch': checkpoint['epoch'],\n",
        "    'best_val_ap50': checkpoint['best_ap'],\n",
        "    'test_metrics': test_metrics,\n",
        "    'training_config': {\n",
        "        'num_epochs': NUM_EPOCHS,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'weight_decay': WEIGHT_DECAY,\n",
        "        'lr_step_size': LR_STEP_SIZE,\n",
        "        'lr_gamma': LR_GAMMA\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(os.path.join(CHECKPOINT_DIR, 'final_results.json'), 'w') as f:\n",
        "    json.dump(final_results, f, indent=2)\n",
        "\n",
        "print(\"\\n‚úÖ All results saved to:\", CHECKPOINT_DIR)\n",
        "print(\"\\nFiles saved:\")\n",
        "print(\"  - best_model.pth\")\n",
        "print(\"  - training_history.csv\")\n",
        "print(\"  - training_curves.png\")\n",
        "print(\"  - predictions_visualization.png\")\n",
        "print(\"  - final_results.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25f3cf98",
      "metadata": {
        "id": "25f3cf98"
      },
      "source": [
        "## 12. Á∏ΩÁµêËàá‰∏ã‰∏ÄÊ≠• (Summary & Next Steps)\n",
        "\n",
        "### üéØ Ë®ìÁ∑¥ÂÆåÊàêÔºÅ\n",
        "\n",
        "### üìä ‰∏ã‰∏ÄÊ≠•ÂÑ™ÂåñÂª∫Ë≠∞Ôºö\n",
        "\n",
        "1. **Ê®°ÂûãÊîπÈÄ≤**\n",
        "   - ÂòóË©¶ ResNet-101 Êàñ ResNeXt ‰ΩúÁÇ∫ backbone\n",
        "   - ‰ΩøÁî® Cascade R-CNN ÊèêÂçáÈ´ò IoU Á≤æÂ∫¶\n",
        "   - ÂØ¶Áèæ Soft-NMS Ê∏õÂ∞ëÈáçÁñäÊ°ÜÂïèÈ°å\n",
        "\n",
        "2. **Ë≥áÊñôÂ¢ûÂº∑**\n",
        "   - Â¢ûÂä† multi-scale training (384, 512, 640, 768)\n",
        "   - ÂØ¶Áèæ CutOut Êàñ MixUp Â¢ûÂº∑\n",
        "   - ‰ΩøÁî® AutoAugment Ëá™ÂãïÊêúÂ∞ãÊúÄ‰Ω≥Á≠ñÁï•\n",
        "\n",
        "3. **Ë®ìÁ∑¥Á≠ñÁï•**\n",
        "   - ÂØ¶Áèæ Test-Time Augmentation (TTA)\n",
        "   - Ë®ìÁ∑¥Â§öÂÄãÊ®°ÂûãÈÄ≤Ë°å ensemble\n",
        "   - ‰ΩøÁî® Hard Negative Mining\n",
        "\n",
        "4. **Ë©ï‰º∞ËàáÂàÜÊûê**\n",
        "   - ÂàÜÊûê False Positives Âíå False Negatives\n",
        "   - ÈáùÂ∞çÂõ∞Èõ£Ê®£Êú¨ÈÄ≤Ë°åÈ°çÂ§ñË®ìÁ∑¥\n",
        "   - ÈÄ≤Ë°åÈåØË™§ÂàÜÊûê‰∏¶Ë™øÊï¥Á≠ñÁï•\n",
        "\n",
        "### üìö ÂèÉËÄÉË≥áÊ∫êÔºö\n",
        "- [Faster R-CNN Paper](https://arxiv.org/abs/1506.01497)\n",
        "- [RSNA Pneumonia Detection Challenge](https://www.kaggle.com/c/rsna-pneumonia-detection-challenge)\n",
        "- [Detectron2 Documentation](https://detectron2.readthedocs.io/)\n",
        "\n",
        "---\n",
        "**Good luck with your competition! üöÄ**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
