{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucas6028/aortic_valve_detection/blob/main/predict_hybrid_wbf_optimized.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85cc7015",
      "metadata": {
        "id": "85cc7015"
      },
      "source": [
        "# üöÄ Hybrid Model Ensemble with WBF (Optimized)\n",
        "\n",
        "## YOLOv8 5-Fold + Faster R-CNN Multi-Model Fusion\n",
        "\n",
        "This notebook combines predictions from:\n",
        "1. **YOLOv8 5-Fold Models**: Using ALL 5 folds for maximum generalization\n",
        "2. **Faster R-CNN Models**: High precision two-stage detector"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60a2aa3c",
      "metadata": {
        "id": "60a2aa3c"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3c857e1d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c857e1d",
        "outputId": "ac13c765-132f-42e7-c754-13e04009dee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Nov 29 08:33:50 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "97d56973",
      "metadata": {
        "id": "97d56973"
      },
      "outputs": [],
      "source": [
        "# Fix encoding issues\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale=True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d19e5f92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d19e5f92",
        "outputId": "169be835-6629-4abb-9394-d4c3f089d536"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ Packages installed successfully\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install ultralytics ensemble-boxes torch torchvision pycocotools -q\n",
        "\n",
        "print(\"‚úÖ Packages installed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d24d54ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d24d54ba",
        "outputId": "67dbb7bd-bdf5-4fe9-8b29-808176b922e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "622f6082",
      "metadata": {
        "id": "622f6082"
      },
      "source": [
        "## 2. Download Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "60c600cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60c600cf",
        "outputId": "fd799df0-2f36-4f4c-afdb-9410007ce1fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Downloading test dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?export=download&id=1drfMaRrfIL0XBRY16Vq9IJ3nJ2Zr9B60\n",
            "From (redirected): https://drive.google.com/uc?export=download&id=1drfMaRrfIL0XBRY16Vq9IJ3nJ2Zr9B60&confirm=t&uuid=751412a9-67dd-4c55-9537-ae3ae63dfdf1\n",
            "To: /content/testing_image.zip\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.83G/1.83G [00:29<00:00, 62.5MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Download complete\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "import os\n",
        "\n",
        "# Download test dataset\n",
        "print(\"üì• Downloading test dataset...\")\n",
        "gdown.download(\n",
        "    \"https://drive.google.com/uc?export=download&id=1drfMaRrfIL0XBRY16Vq9IJ3nJ2Zr9B60\",\n",
        "    \"/content/testing_image.zip\"\n",
        ")\n",
        "print(\"‚úÖ Download complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7554c122",
      "metadata": {
        "id": "7554c122"
      },
      "source": [
        "## 3. Prepare Test Images\n",
        "\n",
        "Split images into two batches to manage Colab RAM limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a5857c2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5857c2a",
        "outputId": "6433c8b6-f7f2-44f3-e626-4a5286e5366f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test root: ./testing_image/testing_image\n",
            "\n",
            "üìä Dataset Validation:\n",
            "  First patient: patient0051\n",
            "  Last patient: patient0100\n",
            "  Total patients: 50\n",
            "‚úÖ Correct test dataset: patient0051-patient0100\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Extract test dataset\n",
        "if not os.path.isdir(\"./testing_image\") and os.path.exists(\"testing_image.zip\"):\n",
        "    os.makedirs(\"./testing_image\", exist_ok=True)\n",
        "    !unzip -q testing_image.zip -d ./testing_image\n",
        "\n",
        "def find_patient_root(root):\n",
        "    \"\"\"Find directory containing patient folders\"\"\"\n",
        "    for dirpath, dirnames, filenames in os.walk(root):\n",
        "        if any(d.startswith(\"patient\") for d in dirnames):\n",
        "            return dirpath\n",
        "    return root\n",
        "\n",
        "TEST_ROOT = find_patient_root(\"./testing_image\")\n",
        "print(f\"Test root: {TEST_ROOT}\")\n",
        "\n",
        "# Validate test dataset (should be patient0051-0100)\n",
        "patient_folders = [f for f in os.listdir(TEST_ROOT) if f.startswith(\"patient\")]\n",
        "if patient_folders:\n",
        "    patient_folders.sort()\n",
        "    first_patient = patient_folders[0]\n",
        "    last_patient = patient_folders[-1]\n",
        "    first_num = int(first_patient.replace(\"patient\", \"\"))\n",
        "    last_num = int(last_patient.replace(\"patient\", \"\"))\n",
        "\n",
        "    print(f\"\\nüìä Dataset Validation:\")\n",
        "    print(f\"  First patient: {first_patient}\")\n",
        "    print(f\"  Last patient: {last_patient}\")\n",
        "    print(f\"  Total patients: {len(patient_folders)}\")\n",
        "\n",
        "    if first_num <= 50:\n",
        "        raise ValueError(\"‚ö†Ô∏è ERROR: Downloaded TRAINING data instead of TEST data!\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Correct test dataset: patient{first_num:04d}-patient{last_num:04d}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c9dc6ae9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9dc6ae9",
        "outputId": "a957dc50-61e2-4931-a254-2eab8349db34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Splitting 16620 images into 2 batches...\n",
            "‚úÖ Batch 1: 8310 images ‚Üí ./datasets/test/images1\n",
            "‚úÖ Batch 2: 8310 images ‚Üí ./datasets/test/images2\n"
          ]
        }
      ],
      "source": [
        "# Split images into two batches\n",
        "dst_root1 = \"./datasets/test/images1\"\n",
        "dst_root2 = \"./datasets/test/images2\"\n",
        "\n",
        "os.makedirs(dst_root1, exist_ok=True)\n",
        "os.makedirs(dst_root2, exist_ok=True)\n",
        "\n",
        "# Collect all image paths\n",
        "all_files = []\n",
        "for patient_folder in os.listdir(TEST_ROOT):\n",
        "    patient_path = os.path.join(TEST_ROOT, patient_folder)\n",
        "    if os.path.isdir(patient_path) and patient_folder.startswith(\"patient\"):\n",
        "        for fname in os.listdir(patient_path):\n",
        "            if fname.endswith(\".png\"):\n",
        "                all_files.append(os.path.join(patient_path, fname))\n",
        "\n",
        "# Sort for reproducibility\n",
        "all_files.sort()\n",
        "\n",
        "# Calculate split point\n",
        "half = len(all_files) // 2\n",
        "\n",
        "# Copy to batch directories\n",
        "print(f\"üì¶ Splitting {len(all_files)} images into 2 batches...\")\n",
        "\n",
        "for f in all_files[:half]:\n",
        "    dst_file = os.path.join(dst_root1, os.path.basename(f))\n",
        "    shutil.copy2(f, dst_file)\n",
        "\n",
        "for f in all_files[half:]:\n",
        "    dst_file = os.path.join(dst_root2, os.path.basename(f))\n",
        "    shutil.copy2(f, dst_file)\n",
        "\n",
        "print(f\"‚úÖ Batch 1: {len(os.listdir(dst_root1))} images ‚Üí {dst_root1}\")\n",
        "print(f\"‚úÖ Batch 2: {len(os.listdir(dst_root2))} images ‚Üí {dst_root2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55492375",
      "metadata": {
        "id": "55492375"
      },
      "source": [
        "## 4. Configuration\n",
        "\n",
        "Configure model paths and WBF parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af21ad47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af21ad47",
        "outputId": "1287d16b-a016-40de-a71d-f3ecfac916f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üìã Configuration:\n",
            "  YOLOv8 K-Fold: /content/drive/MyDrive/AI_CUP_2025/aortic_valve_kfold\n",
            "  Faster R-CNN: /content/drive/MyDrive/AI_CUP_2025/faster_rcnn_checkpoints\n",
            "  Confidence Threshold: 0.0005\n",
            "  Max Output Detections: 50 (Enforcing single object)\n",
            "  TTA Enabled: True\n",
            "  TTA Scales: [1.0, 0.9, 1.1]\n",
            "  TTA Horizontal Flip: True\n",
            "  WBF IoU Threshold: 0.64\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Model paths\n",
        "YOLO_KFOLD_PATH = '/content/drive/MyDrive/AI_CUP_2025/aortic_valve_kfold'\n",
        "FASTER_RCNN_PATH = '/content/drive/MyDrive/AI_CUP_2025/faster_rcnn_checkpoints'\n",
        "\n",
        "# Detection parameters\n",
        "CONF_THRESHOLD = 0.0005  # Low threshold to capture all candidates\n",
        "IMG_SIZE = 640\n",
        "MAX_INPUT_DET = 50      # Max detections per model to feed into WBF\n",
        "MAX_OUTPUT_DET = 50\n",
        "\n",
        "# TTA (Test-Time Augmentation) parameters\n",
        "ENABLE_TTA = False\n",
        "TTA_SCALES = [1.0, 0.9, 1.1]\n",
        "TTA_HORIZONTAL_FLIP = True\n",
        "\n",
        "# WBF parameters\n",
        "WBF_IOU_THR = 0.64\n",
        "WBF_SKIP_BOX_THR = 0.0001\n",
        "\n",
        "# Model weights\n",
        "# Optimized for 5 Folds + Faster R-CNN\n",
        "# Fold 1 (0.99) -> 0.12\n",
        "# Fold 2 (0.986) -> 0.10\n",
        "# Fold 3 (0.985) -> 0.10\n",
        "# Fold 4 (0.985) -> 0.10\n",
        "# Fold 5 (0.977) -> 0.08\n",
        "# Faster R-CNN (0.987) -> 0.50\n",
        "MODEL_WEIGHTS = [0.12, 0.10, 0.10, 0.10, 0.08, 0.50]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üìã Configuration:\")\n",
        "print(f\"  YOLOv8 K-Fold: {YOLO_KFOLD_PATH}\")\n",
        "print(f\"  Faster R-CNN: {FASTER_RCNN_PATH}\")\n",
        "print(f\"  Confidence Threshold: {CONF_THRESHOLD}\")\n",
        "print(f\"  Max Output Detections: {MAX_OUTPUT_DET} (Enforcing single object)\")\n",
        "print(f\"  TTA Enabled: {ENABLE_TTA}\")\n",
        "if ENABLE_TTA:\n",
        "    print(f\"  TTA Scales: {TTA_SCALES}\")\n",
        "    print(f\"  TTA Horizontal Flip: {TTA_HORIZONTAL_FLIP}\")\n",
        "print(f\"  WBF IoU Threshold: {WBF_IOU_THR}\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c8bdb25",
      "metadata": {
        "id": "0c8bdb25"
      },
      "source": [
        "## 5. Load Models\n",
        "\n",
        "Load YOLOv8 5-Fold models and Faster R-CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ed5cb20b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed5cb20b",
        "outputId": "3f15d784-404a-4fad-a295-3925008b4a65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "================================================================================\n",
            "üîÑ Loading Models...\n",
            "================================================================================\n",
            "\n",
            "üì¶ Loading YOLOv8 K-Fold models:\n",
            "  ‚úÖ Fold 1: /content/drive/MyDrive/AI_CUP_2025/aortic_valve_kfold/fold1/weights/best.pt\n",
            "  ‚úÖ Fold 2: /content/drive/MyDrive/AI_CUP_2025/aortic_valve_kfold/fold2/weights/best.pt\n",
            "  ‚úÖ Fold 3: /content/drive/MyDrive/AI_CUP_2025/aortic_valve_kfold/fold3/weights/best.pt\n",
            "  ‚úÖ Fold 4: /content/drive/MyDrive/AI_CUP_2025/aortic_valve_kfold/fold4/weights/best.pt\n",
            "  ‚úÖ Fold 5: /content/drive/MyDrive/AI_CUP_2025/aortic_valve_kfold/fold5/weights/best.pt\n",
            "\n",
            "‚úÖ Loaded 5 YOLOv8 models\n",
            "\n",
            "üì¶ Loading Faster R-CNN model:\n",
            "  ‚úÖ Faster R-CNN: /content/drive/MyDrive/AI_CUP_2025/faster_rcnn_checkpoints/checkpoint_epoch_39.pth\n",
            "     Epoch: 39, AP@0.5: 0.9872\n",
            "\n",
            "================================================================================\n",
            "üìä Total models loaded: 5 YOLO + 1 Faster R-CNN\n",
            "================================================================================\n",
            "\n",
            "\n",
            "üíª Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from ultralytics import YOLO\n",
        "\n",
        "def get_faster_rcnn_model(num_classes=2):\n",
        "    \"\"\"Create Faster R-CNN model architecture\"\"\"\n",
        "    model = fasterrcnn_resnet50_fpn_v2(weights=None)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "def load_all_models():\n",
        "    \"\"\"Load all YOLOv8 fold models and Faster R-CNN model\"\"\"\n",
        "    models = {\n",
        "        'yolo': [],\n",
        "        'faster_rcnn': None\n",
        "    }\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üîÑ Loading Models...\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Load YOLOv8 5-Fold models\n",
        "    print(\"\\nüì¶ Loading YOLOv8 K-Fold models:\")\n",
        "    for fold in range(1, 6):  # CHANGED: Load all 5 folds\n",
        "        model_path = f'{YOLO_KFOLD_PATH}/fold{fold}/weights/best.pt'\n",
        "        if os.path.exists(model_path):\n",
        "            model = YOLO(model_path)\n",
        "            models['yolo'].append(model)\n",
        "            print(f\"  ‚úÖ Fold {fold}: {model_path}\")\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è Fold {fold}: NOT FOUND at {model_path}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Loaded {len(models['yolo'])} YOLOv8 models\")\n",
        "\n",
        "    # Load Faster R-CNN model\n",
        "    print(\"\\nüì¶ Loading Faster R-CNN model:\")\n",
        "    faster_rcnn_checkpoint = os.path.join(FASTER_RCNN_PATH, 'checkpoint_epoch_39.pth')\n",
        "\n",
        "    if os.path.exists(faster_rcnn_checkpoint):\n",
        "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        model = get_faster_rcnn_model(num_classes=2)\n",
        "\n",
        "        checkpoint = torch.load(faster_rcnn_checkpoint, weights_only=False)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        models['faster_rcnn'] = model\n",
        "        print(f\"  ‚úÖ Faster R-CNN: {faster_rcnn_checkpoint}\")\n",
        "        print(f\"     Epoch: {checkpoint['epoch']}, AP@0.5: {checkpoint.get('best_ap', 0.0):.4f}\")\n",
        "    else:\n",
        "        print(f\"  ‚ö†Ô∏è Faster R-CNN: NOT FOUND at {faster_rcnn_checkpoint}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"üìä Total models loaded: {len(models['yolo'])} YOLO + {1 if models['faster_rcnn'] else 0} Faster R-CNN\")\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "    return models\n",
        "\n",
        "# Load all models\n",
        "all_models = load_all_models()\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f\"\\nüíª Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9e3dc7c",
      "metadata": {
        "id": "e9e3dc7c"
      },
      "source": [
        "## 6. Hybrid WBF Prediction Functions\n",
        "\n",
        "Combine predictions from YOLOv8 and Faster R-CNN using WBF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cec03343",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cec03343",
        "outputId": "256d7fdf-2fc3-43b7-df5a-8b74f9b32f38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Hybrid WBF prediction functions with TTA ready\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from ensemble_boxes import weighted_boxes_fusion\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as F\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "def get_image_size(image_path):\n",
        "    \"\"\"Get image dimensions\"\"\"\n",
        "    with Image.open(image_path) as img:\n",
        "        return img.size  # (width, height)\n",
        "\n",
        "def apply_tta_transform(img_tensor, transform_type, scale=1.0):\n",
        "    \"\"\"\n",
        "    Apply Test-Time Augmentation transform to image tensor\n",
        "    \"\"\"\n",
        "    if transform_type == 'original':\n",
        "        return img_tensor\n",
        "    elif transform_type == 'hflip':\n",
        "        return F.hflip(img_tensor)\n",
        "    elif transform_type == 'scale':\n",
        "        if scale != 1.0:\n",
        "            _, h, w = img_tensor.shape\n",
        "            new_h, new_w = int(h * scale), int(w * scale)\n",
        "            return F.resize(img_tensor, [new_h, new_w])\n",
        "        return img_tensor\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown transform type: {transform_type}\")\n",
        "\n",
        "def reverse_tta_boxes(boxes, transform_type, orig_width, orig_height, scale=1.0):\n",
        "    \"\"\"\n",
        "    Reverse TTA transformation on bounding boxes (normalized coordinates)\n",
        "    \"\"\"\n",
        "    if not boxes:\n",
        "        return boxes\n",
        "\n",
        "    reversed_boxes = []\n",
        "    for box in boxes:\n",
        "        x1, y1, x2, y2 = box\n",
        "\n",
        "        if transform_type == 'hflip':\n",
        "            # Flip x coordinates\n",
        "            x1_new = 1.0 - x2\n",
        "            x2_new = 1.0 - x1\n",
        "            reversed_boxes.append([x1_new, y1, x2_new, y2])\n",
        "        elif transform_type == 'scale':\n",
        "            # Boxes are already normalized, no change needed for scale\n",
        "            reversed_boxes.append([x1, y1, x2, y2])\n",
        "        else:\n",
        "            reversed_boxes.append([x1, y1, x2, y2])\n",
        "\n",
        "    return reversed_boxes\n",
        "\n",
        "def predict_yolo_single_image_with_tta(yolo_models, img_path, orig_width, orig_height):\n",
        "    \"\"\"\n",
        "    Get predictions from all YOLOv8 fold models with TTA for a single image\n",
        "    \"\"\"\n",
        "    boxes_list = []\n",
        "    scores_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    # Load image with cv2 for YOLO\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    if img_cv2 is None:\n",
        "        return [], [], []\n",
        "\n",
        "    # Generate TTA variants\n",
        "    tta_configs = []\n",
        "\n",
        "    if ENABLE_TTA:\n",
        "        # Original + scales\n",
        "        for scale in TTA_SCALES:\n",
        "            tta_configs.append(('scale', scale))\n",
        "\n",
        "        # Horizontal flip (only at original scale to avoid too many variants)\n",
        "        if TTA_HORIZONTAL_FLIP:\n",
        "            tta_configs.append(('hflip', 1.0))\n",
        "    else:\n",
        "        # No TTA, just original\n",
        "        tta_configs.append(('original', 1.0))\n",
        "\n",
        "    for model in yolo_models:\n",
        "        for transform_type, scale in tta_configs:\n",
        "            # Prepare image source\n",
        "            if transform_type == 'hflip':\n",
        "                # Flip image horizontally\n",
        "                source_img = cv2.flip(img_cv2, 1)\n",
        "                img_size = IMG_SIZE\n",
        "            elif transform_type == 'scale':\n",
        "                # YOLO handles scaling via imgsz, pass original image\n",
        "                source_img = img_cv2\n",
        "                img_size = int(IMG_SIZE * scale)\n",
        "            else:\n",
        "                source_img = img_cv2\n",
        "                img_size = IMG_SIZE\n",
        "\n",
        "            # Predict with YOLO\n",
        "            results = model.predict(\n",
        "                source=source_img,\n",
        "                save=False,\n",
        "                imgsz=img_size,\n",
        "                device=0,\n",
        "                verbose=False,\n",
        "                conf=CONF_THRESHOLD,\n",
        "                iou=0.5,\n",
        "                max_det=MAX_INPUT_DET,\n",
        "                flipud=False,\n",
        "                fliplr=False  # We handle flipping manually\n",
        "            )\n",
        "\n",
        "            result = results[0]\n",
        "            boxes = result.boxes\n",
        "\n",
        "            fold_boxes = []\n",
        "            fold_scores = []\n",
        "            fold_labels = []\n",
        "\n",
        "            if len(boxes.cls) > 0:\n",
        "                for j in range(len(boxes.cls)):\n",
        "                    x1, y1, x2, y2 = boxes.xyxy[j].tolist()\n",
        "\n",
        "                    # Normalize to 0-1\n",
        "                    x1_norm = x1 / orig_width\n",
        "                    y1_norm = y1 / orig_height\n",
        "                    x2_norm = x2 / orig_width\n",
        "                    y2_norm = y2 / orig_height\n",
        "\n",
        "                    fold_boxes.append([x1_norm, y1_norm, x2_norm, y2_norm])\n",
        "                    fold_scores.append(boxes.conf[j].item())\n",
        "                    fold_labels.append(int(boxes.cls[j].item()))\n",
        "\n",
        "            # Reverse TTA transformation if needed\n",
        "            fold_boxes = reverse_tta_boxes(fold_boxes, transform_type, orig_width, orig_height, scale)\n",
        "\n",
        "            boxes_list.append(fold_boxes)\n",
        "            scores_list.append(fold_scores)\n",
        "            labels_list.append(fold_labels)\n",
        "\n",
        "    return boxes_list, scores_list, labels_list\n",
        "\n",
        "def predict_faster_rcnn_single_image_with_tta(model, img_path, orig_width, orig_height, device):\n",
        "    \"\"\"\n",
        "    Get predictions from Faster R-CNN model with TTA for a single image\n",
        "    \"\"\"\n",
        "    if model is None:\n",
        "        return [], [], []\n",
        "\n",
        "    boxes_list = []\n",
        "    scores_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    # Load image\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    transform = T.ToTensor()\n",
        "    img_tensor = transform(img)\n",
        "\n",
        "    # Generate TTA variants\n",
        "    tta_configs = []\n",
        "\n",
        "    if ENABLE_TTA:\n",
        "        # Original + scales\n",
        "        for scale in TTA_SCALES:\n",
        "            tta_configs.append(('scale', scale))\n",
        "\n",
        "        # Horizontal flip\n",
        "        if TTA_HORIZONTAL_FLIP:\n",
        "            tta_configs.append(('hflip', 1.0))\n",
        "    else:\n",
        "        # No TTA, just original\n",
        "        tta_configs.append(('original', 1.0))\n",
        "\n",
        "    for transform_type, scale in tta_configs:\n",
        "        # Apply TTA transform\n",
        "        if transform_type == 'scale' and scale != 1.0:\n",
        "            transformed_tensor = apply_tta_transform(img_tensor, 'scale', scale)\n",
        "        elif transform_type == 'hflip':\n",
        "            transformed_tensor = apply_tta_transform(img_tensor, 'hflip')\n",
        "        else:\n",
        "            transformed_tensor = img_tensor\n",
        "\n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            predictions = model([transformed_tensor.to(device)])\n",
        "\n",
        "        prediction = predictions[0]\n",
        "        boxes = prediction['boxes'].cpu().numpy()\n",
        "        scores = prediction['scores'].cpu().numpy()\n",
        "        labels = prediction['labels'].cpu().numpy()\n",
        "\n",
        "        # Get transformed image dimensions\n",
        "        _, h_trans, w_trans = transformed_tensor.shape\n",
        "\n",
        "        # Filter by confidence and normalize\n",
        "        variant_boxes = []\n",
        "        variant_scores = []\n",
        "        variant_labels = []\n",
        "\n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            if score >= CONF_THRESHOLD and label == 1:  # Class 1 = aortic_valve\n",
        "                x1, y1, x2, y2 = box\n",
        "\n",
        "                # Normalize to transformed image dimensions first\n",
        "                x1_norm = x1 / w_trans\n",
        "                y1_norm = y1 / h_trans\n",
        "                x2_norm = x2 / w_trans\n",
        "                y2_norm = y2 / h_trans\n",
        "\n",
        "                variant_boxes.append([x1_norm, y1_norm, x2_norm, y2_norm])\n",
        "                variant_scores.append(float(score))\n",
        "                variant_labels.append(0)  # Convert to 0-indexed for WBF\n",
        "\n",
        "        # Reverse TTA transformation\n",
        "        variant_boxes = reverse_tta_boxes(variant_boxes, transform_type, orig_width, orig_height, scale)\n",
        "\n",
        "        boxes_list.append(variant_boxes)\n",
        "        scores_list.append(variant_scores)\n",
        "        labels_list.append(variant_labels)\n",
        "\n",
        "    return boxes_list, scores_list, labels_list\n",
        "\n",
        "def predict_batch_hybrid_wbf(models, source_path, output_file):\n",
        "    \"\"\"\n",
        "    Predict on a batch of images using hybrid YOLOv8 + Faster R-CNN ensemble with TTA and WBF\n",
        "    \"\"\"\n",
        "    # Get all image files\n",
        "    image_files = [f for f in os.listdir(source_path) if f.endswith('.png')]\n",
        "    image_files.sort()\n",
        "\n",
        "    # Calculate correct weights for TTA\n",
        "    # We need to expand MODEL_WEIGHTS to match the number of TTA variants per model\n",
        "\n",
        "    # Determine number of TTA variants\n",
        "    num_tta_variants = 1\n",
        "    if ENABLE_TTA:\n",
        "        num_tta_variants = len(TTA_SCALES) + (1 if TTA_HORIZONTAL_FLIP else 0)\n",
        "\n",
        "    # Expand weights\n",
        "    # MODEL_WEIGHTS structure: [YOLO_Fold1, YOLO_Fold2, ..., Faster_RCNN]\n",
        "    expanded_weights = []\n",
        "\n",
        "    # Add weights for YOLO models\n",
        "    for i in range(len(models['yolo'])):\n",
        "        weight = MODEL_WEIGHTS[i] if i < len(MODEL_WEIGHTS) else 1.0\n",
        "        expanded_weights.extend([weight] * num_tta_variants)\n",
        "\n",
        "    # Add weights for Faster R-CNN\n",
        "    if models['faster_rcnn'] is not None:\n",
        "        frcnn_idx = len(models['yolo'])\n",
        "        weight = MODEL_WEIGHTS[frcnn_idx] if frcnn_idx < len(MODEL_WEIGHTS) else 1.0\n",
        "        expanded_weights.extend([weight] * num_tta_variants)\n",
        "\n",
        "    tta_info = \"\"\n",
        "    if ENABLE_TTA:\n",
        "        tta_info = f\" with TTA ({num_tta_variants} variants per model)\"\n",
        "\n",
        "    print(f\"\\nüîÆ Processing {len(image_files)} images with Hybrid WBF ensemble{tta_info}...\")\n",
        "    print(f\"   Models: {len(models['yolo'])} YOLOv8 + {1 if models['faster_rcnn'] else 0} Faster R-CNN\")\n",
        "    print(f\"   Total prediction lists per image: {len(expanded_weights)}\")\n",
        "\n",
        "    if ENABLE_TTA:\n",
        "        print(f\"   TTA Scales: {TTA_SCALES}\")\n",
        "        print(f\"   TTA Flip: {TTA_HORIZONTAL_FLIP}\")\n",
        "\n",
        "    final_predictions = {}\n",
        "\n",
        "    for img_idx, img_file in enumerate(tqdm(image_files, desc=\"Predicting\")):\n",
        "        img_path = os.path.join(source_path, img_file)\n",
        "        filename = img_file.split('.png')[0]\n",
        "\n",
        "        # Get original image size\n",
        "        orig_width, orig_height = get_image_size(img_path)\n",
        "\n",
        "        # Collect predictions from all models and TTA variants\n",
        "        all_boxes_list = []\n",
        "        all_scores_list = []\n",
        "        all_labels_list = []\n",
        "\n",
        "        # 1. Get YOLOv8 predictions with TTA\n",
        "        yolo_boxes, yolo_scores, yolo_labels = predict_yolo_single_image_with_tta(\n",
        "            models['yolo'], img_path, orig_width, orig_height\n",
        "        )\n",
        "        all_boxes_list.extend(yolo_boxes)\n",
        "        all_scores_list.extend(yolo_scores)\n",
        "        all_labels_list.extend(yolo_labels)\n",
        "\n",
        "        # 2. Get Faster R-CNN predictions with TTA\n",
        "        if models['faster_rcnn'] is not None:\n",
        "            frcnn_boxes, frcnn_scores, frcnn_labels = predict_faster_rcnn_single_image_with_tta(\n",
        "                models['faster_rcnn'], img_path, orig_width, orig_height, device\n",
        "            )\n",
        "            all_boxes_list.extend(frcnn_boxes)\n",
        "            all_scores_list.extend(frcnn_scores)\n",
        "            all_labels_list.extend(frcnn_labels)\n",
        "\n",
        "        # 3. Apply WBF to fuse all predictions (including TTA variants)\n",
        "        if any(len(boxes) > 0 for boxes in all_boxes_list):\n",
        "            # Ensure weights match boxes list length\n",
        "            current_weights = expanded_weights\n",
        "            if len(all_boxes_list) != len(expanded_weights):\n",
        "                # Fallback if mismatch (should not happen if logic is correct)\n",
        "                current_weights = None\n",
        "\n",
        "            fused_boxes, fused_scores, fused_labels = weighted_boxes_fusion(\n",
        "                all_boxes_list,\n",
        "                all_scores_list,\n",
        "                all_labels_list,\n",
        "                weights=current_weights,\n",
        "                iou_thr=WBF_IOU_THR,\n",
        "                skip_box_thr=WBF_SKIP_BOX_THR\n",
        "            )\n",
        "\n",
        "            # Sort by confidence descending\n",
        "            sorted_indices = np.argsort(fused_scores)[::-1]\n",
        "\n",
        "            # Keep only top K (Single object constraint)\n",
        "            if MAX_OUTPUT_DET > 0:\n",
        "                sorted_indices = sorted_indices[:MAX_OUTPUT_DET]\n",
        "\n",
        "            # Convert normalized coordinates back to pixels\n",
        "            final_predictions[filename] = []\n",
        "            for idx in sorted_indices:\n",
        "                box = fused_boxes[idx]\n",
        "                score = fused_scores[idx]\n",
        "                label = fused_labels[idx]\n",
        "\n",
        "                x1 = int(box[0] * orig_width)\n",
        "                y1 = int(box[1] * orig_height)\n",
        "                x2 = int(box[2] * orig_width)\n",
        "                y2 = int(box[3] * orig_height)\n",
        "\n",
        "                final_predictions[filename].append({\n",
        "                    'label': int(label),\n",
        "                    'conf': float(score),\n",
        "                    'box': [x1, y1, x2, y2]\n",
        "                })\n",
        "\n",
        "    # Write predictions to file\n",
        "    print(f\"\\nüíæ Writing predictions to {output_file}...\")\n",
        "    with open(output_file, 'w') as f:\n",
        "        for filename, predictions in final_predictions.items():\n",
        "            for pred in predictions:\n",
        "                line = f\"{filename} {pred['label']} {pred['conf']:.4f} {pred['box'][0]} {pred['box'][1]} {pred['box'][2]} {pred['box'][3]}\\n\"\n",
        "                f.write(line)\n",
        "\n",
        "    num_detections = sum(len(preds) for preds in final_predictions.values())\n",
        "    num_images_with_detections = len(final_predictions)\n",
        "\n",
        "    print(f\"‚úÖ Predictions saved!\")\n",
        "    print(f\"   Total detections: {num_detections}\")\n",
        "    print(f\"   Images with detections: {num_images_with_detections}\")\n",
        "    if num_images_with_detections > 0:\n",
        "        print(f\"   Average detections per image: {num_detections/num_images_with_detections:.2f}\")\n",
        "\n",
        "print(\"‚úÖ Hybrid WBF prediction functions with TTA ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ec81916",
      "metadata": {
        "id": "6ec81916"
      },
      "source": [
        "## 7. Predict Batch 1 with Hybrid WBF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "11b737e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11b737e8",
        "outputId": "d583d55b-cf51-47ac-b231-9cd511abf734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üöÄ Starting Batch 1 Prediction (Hybrid WBF + TTA)\n",
            "================================================================================\n",
            "\n",
            "üîÆ Processing 8310 images with Hybrid WBF ensemble with TTA (4 variants per model)...\n",
            "   Models: 5 YOLOv8 + 1 Faster R-CNN\n",
            "   Total prediction lists per image: 24\n",
            "   TTA Scales: [1.0, 0.9, 1.1]\n",
            "   TTA Flip: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8310/8310 [2:21:55<00:00,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Writing predictions to /content/predict_txt/images1_hybrid_wbf_tta.txt...\n",
            "‚úÖ Predictions saved!\n",
            "   Total detections: 7156\n",
            "   Images with detections: 2807\n",
            "   Average detections per image: 2.55\n",
            "\n",
            "‚úÖ Batch 1 complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Create output directory\n",
        "os.makedirs('/content/predict_txt', exist_ok=True)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üöÄ Starting Batch 1 Prediction (Hybrid WBF + TTA)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "predict_batch_hybrid_wbf(\n",
        "    models=all_models,\n",
        "    source_path=dst_root1,\n",
        "    output_file='/content/predict_txt/images1_hybrid_wbf_tta.txt'\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Batch 1 complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "202cb4ee",
      "metadata": {
        "id": "202cb4ee"
      },
      "source": [
        "## 8. Memory Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4851b5b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4851b5b0",
        "outputId": "169d3c42-d3d4-43aa-b107-6fe39a6d7b43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßπ Cleaning up memory...\n",
            "‚úÖ Memory cleared\n"
          ]
        }
      ],
      "source": [
        "# Clear memory between batches\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "print(\"üßπ Cleaning up memory...\")\n",
        "\n",
        "# Clear YOLO models\n",
        "if 'all_models' in locals():\n",
        "    if 'yolo' in all_models:\n",
        "        all_models['yolo'].clear()\n",
        "    if 'faster_rcnn' in all_models and all_models['faster_rcnn'] is not None:\n",
        "        del all_models['faster_rcnn']\n",
        "    del all_models\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"‚úÖ Memory cleared\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68ff2803",
      "metadata": {
        "id": "68ff2803"
      },
      "source": [
        "## 9. Reload Models for Batch 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1a55ea22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a55ea22",
        "outputId": "e22e0ca7-ce34-490a-9c33-b812135a6d8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üîÑ Reloading models for Batch 2...\n",
            "================================================================================\n",
            "================================================================================\n",
            "üîÑ Loading Models...\n",
            "================================================================================\n",
            "\n",
            "üì¶ Loading YOLOv8 K-Fold models:\n",
            "  ‚úÖ Fold 1: /content/drive/MyDrive/AI_CUP_2025/aortic_valve_kfold/fold1/weights/best.pt\n",
            "  ‚úÖ Fold 2: /content/drive/MyDrive/AI_CUP_2025/aortic_valve_kfold/fold2/weights/best.pt\n",
            "  ‚úÖ Fold 3: /content/drive/MyDrive/AI_CUP_2025/aortic_valve_kfold/fold3/weights/best.pt\n",
            "  ‚úÖ Fold 4: /content/drive/MyDrive/AI_CUP_2025/aortic_valve_kfold/fold4/weights/best.pt\n",
            "  ‚úÖ Fold 5: /content/drive/MyDrive/AI_CUP_2025/aortic_valve_kfold/fold5/weights/best.pt\n",
            "\n",
            "‚úÖ Loaded 5 YOLOv8 models\n",
            "\n",
            "üì¶ Loading Faster R-CNN model:\n",
            "  ‚úÖ Faster R-CNN: /content/drive/MyDrive/AI_CUP_2025/faster_rcnn_checkpoints/checkpoint_epoch_39.pth\n",
            "     Epoch: 39, AP@0.5: 0.9872\n",
            "\n",
            "================================================================================\n",
            "üìä Total models loaded: 5 YOLO + 1 Faster R-CNN\n",
            "================================================================================\n",
            "\n",
            "\n",
            "‚úÖ Models reloaded successfully\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"üîÑ Reloading models for Batch 2...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "all_models = load_all_models()\n",
        "\n",
        "print(\"\\n‚úÖ Models reloaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caba2399",
      "metadata": {
        "id": "caba2399"
      },
      "source": [
        "## 10. Predict Batch 2 with Hybrid WBF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3442638b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3442638b",
        "outputId": "545b637f-32da-4284-847a-e3f060221457"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üöÄ Starting Batch 2 Prediction (Hybrid WBF + TTA)\n",
            "================================================================================\n",
            "\n",
            "üîÆ Processing 8310 images with Hybrid WBF ensemble with TTA (4 variants per model)...\n",
            "   Models: 5 YOLOv8 + 1 Faster R-CNN\n",
            "   Total prediction lists per image: 24\n",
            "   TTA Scales: [1.0, 0.9, 1.1]\n",
            "   TTA Flip: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8310/8310 [2:22:01<00:00,  1.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Writing predictions to /content/predict_txt/images2_hybrid_wbf_tta.txt...\n",
            "‚úÖ Predictions saved!\n",
            "   Total detections: 7126\n",
            "   Images with detections: 2720\n",
            "   Average detections per image: 2.62\n",
            "\n",
            "‚úÖ Batch 2 complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"üöÄ Starting Batch 2 Prediction (Hybrid WBF + TTA)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "predict_batch_hybrid_wbf(\n",
        "    models=all_models,\n",
        "    source_path=dst_root2,\n",
        "    output_file='/content/predict_txt/images2_hybrid_wbf_tta.txt'\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Batch 2 complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53f7c253",
      "metadata": {
        "id": "53f7c253"
      },
      "source": [
        "## 11. Merge Results and Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "951d32a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "951d32a4",
        "outputId": "f54113a1-3d46-4d7f-d9bf-eb9a60c4b6fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üîó Merging batch results...\n",
            "================================================================================\n",
            "  ‚úÖ Merged: /content/predict_txt/images1_hybrid_wbf_tta.txt\n",
            "  ‚úÖ Merged: /content/predict_txt/images2_hybrid_wbf_tta.txt\n",
            "\n",
            "‚úÖ Final submission file: /content/predict_txt/submission_hybrid_wbf_optimized.txt\n",
            "\n",
            "================================================================================\n",
            "üìä Final Statistics (Optimized):\n",
            "================================================================================\n",
            "  Total detections: 14282\n",
            "  Images with detections: 5527\n",
            "  Average detections per image: 2.58\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Merge two batch result files\n",
        "file1 = \"/content/predict_txt/images1_hybrid_wbf_tta.txt\"\n",
        "file2 = \"/content/predict_txt/images2_hybrid_wbf_tta.txt\"\n",
        "output = \"/content/predict_txt/submission_hybrid_wbf_optimized.txt\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üîó Merging batch results...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "with open(output, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for f in [file1, file2]:\n",
        "        if os.path.exists(f):\n",
        "            with open(f, \"r\", encoding=\"utf-8\") as fin:\n",
        "                fout.writelines(fin.readlines())\n",
        "            print(f\"  ‚úÖ Merged: {f}\")\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è Not found: {f}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Final submission file: {output}\")\n",
        "\n",
        "# Count statistics\n",
        "def count_predictions(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        return 0, 0\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    unique_images = set()\n",
        "    for line in lines:\n",
        "        filename = line.split()[0]\n",
        "        unique_images.add(filename)\n",
        "\n",
        "    return len(lines), len(unique_images)\n",
        "\n",
        "total_boxes, total_images = count_predictions(output)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä Final Statistics (Optimized):\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"  Total detections: {total_boxes}\")\n",
        "print(f\"  Images with detections: {total_images}\")\n",
        "if total_images > 0:\n",
        "    print(f\"  Average detections per image: {total_boxes/total_images:.2f}\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a665ae5f",
      "metadata": {
        "id": "a665ae5f"
      },
      "source": [
        "## 12. Download Submission File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "72c81d8f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "72c81d8f",
        "outputId": "4485d3a0-dcc6-4649-82e8-a296b40ecf45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Downloading submission file (Optimized)...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_acfd2ffd-cc56-4bf4-ae32-05b391d0de29\", \"submission_hybrid_wbf_optimized.txt\", 598931)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Download complete!\n"
          ]
        }
      ],
      "source": [
        "# Download the final submission file\n",
        "from google.colab import files\n",
        "\n",
        "print(\"üì• Downloading submission file (Optimized)...\")\n",
        "files.download('/content/predict_txt/submission_hybrid_wbf_optimized.txt')\n",
        "print(\"‚úÖ Download complete!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
